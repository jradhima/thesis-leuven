{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79790c26-db8a-40ab-addd-b9524663b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61920596-a725-41eb-bacc-ed41201d6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('data.db')\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "articles = [*cur.execute(\"select * from article;\")]\n",
    "companies = [*cur.execute(\"select * from company;\")]\n",
    "a2c = [*cur.execute(\"select * from article_company;\")]\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32b46e-458e-4c36-bdf8-1f672ec2ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.DataFrame(articles, columns=['idx', 'title', 'text', 'href', 'date']).set_index('idx')\n",
    "a2c = pd.DataFrame(a2c, columns=['article_id', 'company_id'])\n",
    "companies = pd.DataFrame(companies, columns=['idx', 'name', 'ticker']).set_index('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ed24b-b0f3-43b5-b84c-a28dde0bb1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['text_size'] = articles['text'].apply(lambda x: len(x.split()))\n",
    "articles['date'] = pd.to_datetime(articles['date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd2922-6658-4002-b460-8a6cfc583d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = articles['date'] > '2018-01-01'\n",
    "articles = articles[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2a908-4d4d-42db-b4ad-9be3cfb3104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = articles['text_size'] > 50\n",
    "articles = articles[mask]\n",
    "mask = articles['text_size'] < 300\n",
    "articles = articles[mask]\n",
    "articles['text_size'].hist(bins=50, figsize=(15,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138141b5-7656-4613-9a49-928db70ce544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraphs_sentiment(article, model, tokenizer, chunksize=512):\n",
    "    \n",
    "    cleaned_paragraphs = clean_article(article)\n",
    "    \n",
    "    input_chunks = []\n",
    "    mask_chunks = []\n",
    "    for i in range(len(cleaned_paragraphs)):\n",
    "        token = tokenizer.encode_plus(cleaned_paragraphs[i], add_special_tokens=False, return_tensors='pt')\n",
    "        input_chunks.append(token['input_ids'][0])\n",
    "        mask_chunks.append(token['attention_mask'][0])\n",
    "        \n",
    "    model_input = preprocess_input(chunksize, input_chunks, mask_chunks)\n",
    "    \n",
    "    output = model(**model_input)\n",
    "    probs = torch.nn.functional.softmax(output[0], dim=-1).mean(dim=0)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a524b0e-aed1-4f79-ad3b-07bdd29aebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article(article):\n",
    "    elements_to_pop = []\n",
    "    article_chunks = article.split('\\n')\n",
    "\n",
    "    for i, chunk in enumerate(article_chunks):\n",
    "        if re.search(\"\\S*@\\S*\", chunk) != None or chunk == '':\n",
    "            elements_to_pop.append(i)\n",
    "\n",
    "    for i in reversed(elements_to_pop):\n",
    "        article_chunks.pop(i)\n",
    "    \n",
    "    return article_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa923fa-3684-43f0-ab5d-2ad905463902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(chunksize, input_id_chunks, mask_chunks):\n",
    "    alpha = torch.tensor([101])\n",
    "    beta = torch.tensor([1])\n",
    "    input_tensors = []\n",
    "    mask_tensors = []\n",
    "\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        if len(input_id_chunks[i]) + len(alpha) < 512:\n",
    "            alpha = torch.cat([alpha, input_id_chunks[i]])\n",
    "            beta = torch.cat([beta, mask_chunks[i]])\n",
    "        else:\n",
    "            alpha = torch.cat([alpha, torch.tensor([102]), torch.tensor([0] * (chunksize - len(alpha) - 1))])\n",
    "            beta = torch.cat([beta, torch.tensor([1]), torch.tensor([0] * (chunksize - len(beta) - 1))])\n",
    "\n",
    "            input_tensors.append(alpha)\n",
    "            mask_tensors.append(beta)\n",
    "            \n",
    "            alpha = torch.cat([torch.tensor([101]), input_id_chunks[i]])\n",
    "            beta = torch.cat([torch.tensor([1]), mask_chunks[i]])\n",
    "\n",
    "    if input_tensors == [] or len(input_tensors[-1]) != 512:\n",
    "        alpha = torch.cat([alpha, torch.tensor([102]), torch.tensor([0] * (chunksize - len(alpha) - 1))])\n",
    "        beta = torch.cat([beta, torch.tensor([1]), torch.tensor([0] * (chunksize - len(beta) - 1))])\n",
    "\n",
    "        input_tensors.append(alpha)\n",
    "        mask_tensors.append(beta)\n",
    "    \n",
    "    input_ids = torch.stack(input_tensors)\n",
    "    attention_mask = torch.stack(mask_tensors)\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids': input_ids.long(),\n",
    "        'attention_mask': attention_mask.int()\n",
    "    }\n",
    "    \n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebf212-6551-4dc2-9c3f-5d81ef899ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_sentiment = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0799786-88d2-4a52-8e51-b0fcf25408ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = articles['text'].sample().item()\n",
    "some_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02069b7-ebe1-45c6-a2f9-3636ac797037",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = paragraphs_sentiment(some_text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd42706-5bd4-49aa-b70e-84fa8f0e41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "['negative', 'neutral', 'positive'][torch.argmax(probs).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22eb744-1060-4e25-9440-f2dc0cd9516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs @ torch.tensor([1, -1, 0]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379445a7-7687-4205-a518-f20416a2b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "for row in articles.sample(10).itertuples():\n",
    "    #probs = paragraphs_sentiment(row.text, model, tokenizer)\n",
    "    #sentiments.append((probs, probs @ torch.tensor([1, -1, 0]).float()))\n",
    "\n",
    "sentiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
